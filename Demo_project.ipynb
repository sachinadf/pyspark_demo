{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7q0IlpFNNa0J",
        "outputId": "674178fd-a318-4fec-aa2b-a9dfb5850453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q !wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "XXdBc6k5ObUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "MVHVIkJQOdIN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "PHQpsOaQOe7F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "KXiajfM1OgjG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Spark Session"
      ],
      "metadata": {
        "id": "AswHxV2MZfOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "spark=SparkSession.builder.appName('Test').master('local[3]').enableHiveSupport().getOrCreate()\n",
        "print(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmDR6LTjOiEW",
        "outputId": "57b3d60f-dfb8-41ec-dce0-c2a3284196fa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7f05bf41c390>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating RDD. Count word in files using map, flatmap, reduceByKey"
      ],
      "metadata": {
        "id": "MbyxNCmVYZrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=spark.sparkContext.textFile('/content/sample_file.txt')\\\n",
        "        .flatMap(lambda x:x.split(' '))\\\n",
        "        .map(lambda x:(x,1))\\\n",
        "        .reduceByKey(lambda x,y:x+y)\n",
        "\n",
        "for (word, count) in rdd.collect():\n",
        "          print(\"%s: %i\" % (word, count))"
      ],
      "metadata": {
        "id": "mwhIGQU8Y1k1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2465a04e-c293-428e-db4e-5bddb16c6d52"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "refers: 1\n",
            "large,: 1\n",
            "diverse: 1\n",
            "sets: 1\n",
            "of: 4\n",
            ": 6\n",
            "at: 2\n",
            "ever-increasing: 1\n",
            "It: 1\n",
            "velocity: 1\n",
            "speed: 1\n",
            "is: 1\n",
            "created: 1\n",
            "collected,: 1\n",
            "scope: 1\n",
            "points: 1\n",
            "covered: 1\n",
            "as: 1\n",
            "\"three: 1\n",
            "v's\": 1\n",
            "mining: 1\n",
            "in: 1\n",
            "multiple: 1\n",
            "formats.: 1\n",
            "Big: 2\n",
            "data: 4\n",
            "to: 1\n",
            "the: 6\n",
            "information: 1\n",
            "that: 1\n",
            "grow: 1\n",
            "rates.: 1\n",
            "encompasses: 1\n",
            "volume: 1\n",
            "information,: 1\n",
            "or: 2\n",
            "which: 1\n",
            "it: 1\n",
            "and: 3\n",
            "variety: 1\n",
            "being: 1\n",
            "(known: 1\n",
            "big: 1\n",
            "data).: 1\n",
            "often: 1\n",
            "comes: 1\n",
            "from: 1\n",
            "arrives: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating dataframe"
      ],
      "metadata": {
        "id": "xYefXnUVaErV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df = spark.read \\\n",
        "        .option('inferSchema', 'true') \\\n",
        "        .option('header', 'true') \\\n",
        "        .csv('/content/walmart_stock.csv')\n",
        "\n",
        "print(type(stock_df))\n",
        "stock_df.printSchema()\n",
        "# stock_df.select(round('Adj Close',2)).show?()\n",
        "stock_df.show()"
      ],
      "metadata": {
        "id": "wMnzLAWmaMbm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcbba4cb-eb71-4a7a-90ef-20d794a8c7d7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n",
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            "\n",
            "+----------+------------------+------------------+------------------+------------------+--------+------------------+\n",
            "|      Date|              Open|              High|               Low|             Close|  Volume|         Adj Close|\n",
            "+----------+------------------+------------------+------------------+------------------+--------+------------------+\n",
            "|2012-01-03|         59.970001|         61.060001|         59.869999|         60.330002|12668800|52.619234999999996|\n",
            "|2012-01-04|60.209998999999996|         60.349998|         59.470001|59.709998999999996| 9593300|         52.078475|\n",
            "|2012-01-05|         59.349998|         59.619999|         58.369999|         59.419998|12768200|         51.825539|\n",
            "|2012-01-06|         59.419998|         59.450001|         58.869999|              59.0| 8069400|          51.45922|\n",
            "|2012-01-09|         59.029999|         59.549999|         58.919998|             59.18| 6679300|51.616215000000004|\n",
            "|2012-01-10|             59.43|59.709998999999996|             58.98|59.040001000000004| 6907300|         51.494109|\n",
            "|2012-01-11|         59.060001|         59.529999|59.040001000000004|         59.400002| 6365600|         51.808098|\n",
            "|2012-01-12|59.790001000000004|              60.0|         59.400002|              59.5| 7236400|51.895315999999994|\n",
            "|2012-01-13|             59.18|59.610001000000004|59.009997999999996|59.540001000000004| 7729300|51.930203999999996|\n",
            "|2012-01-17|         59.869999|60.110001000000004|             59.52|         59.849998| 8500000|         52.200581|\n",
            "|2012-01-18|59.790001000000004|         60.029999|         59.650002|60.009997999999996| 5911400|         52.340131|\n",
            "|2012-01-19|             59.93|             60.73|             59.75|60.610001000000004| 9234600|         52.863447|\n",
            "|2012-01-20|             60.75|             61.25|         60.669998|61.009997999999996|10378800|53.212320999999996|\n",
            "|2012-01-23|         60.810001|             60.98|60.509997999999996|             60.91| 7134100|         53.125104|\n",
            "|2012-01-24|             60.75|              62.0|             60.75|61.389998999999996| 7362800| 53.54375400000001|\n",
            "|2012-01-25|             61.18|61.610001000000004|61.040001000000004|         61.470001| 5915800| 53.61353100000001|\n",
            "|2012-01-26|         61.799999|             61.84|             60.77|         60.970001| 7436200|         53.177436|\n",
            "|2012-01-27|60.860001000000004|         61.119999|60.540001000000004|60.709998999999996| 6287300|         52.950665|\n",
            "|2012-01-30|         60.470001|             61.32|         60.349998|         61.299999| 7636900|53.465256999999994|\n",
            "|2012-01-31|         61.529999|             61.57|         60.580002|61.360001000000004| 9761500|53.517590000000006|\n",
            "+----------+------------------+------------------+------------------+------------------+--------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling bad records"
      ],
      "metadata": {
        "id": "-2tVJ9SCdOuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Age\", IntegerType(), True),\n",
        "    StructField(\"_corrupt_record\", StringType(), True)\n",
        "])\n",
        "#.option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\\ PERMISSIVE, DROPMALFORMED\n",
        "df1 = spark.read\\\n",
        "         .option(\"mode\", \"PERMISSIVE\")\\\n",
        "         .schema(schema)\\\n",
        "         .option('header', 'true') \\\n",
        "         .csv('/content/bad_rec_file.csv')\n",
        "df1.printSchema()\n",
        "df1.show()\n",
        "\n",
        "df2 = spark.read\\\n",
        "         .option(\"mode\", \"DROPMALFORMED\")\\\n",
        "         .schema(schema)\\\n",
        "         .option('header', 'true') \\\n",
        "         .csv('/content/bad_rec_file.csv')\n",
        "# df2.printSchema()\n",
        "# df2.show()\n",
        "\n",
        "df3 = spark.read\\\n",
        "         .option(\"mode\", \"FAILFAST\")\\\n",
        "         .schema(schema)\\\n",
        "         .option('header', 'true') \\\n",
        "         .csv('/content/bad_rec_file.csv')\n",
        "# df3.printSchema()\n",
        "# df3.show()"
      ],
      "metadata": {
        "id": "DcUlzoNRdLy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling NULL values"
      ],
      "metadata": {
        "id": "d3CsU3oemKS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df4 = spark.read \\\n",
        "        .option('inferSchema', 'true') \\\n",
        "        .option('header', 'true') \\\n",
        "        .csv('/content/null_val.csv')\n",
        "\n",
        "df4.show()\n",
        "print('Using isnotNull')\n",
        "df4.filter((df4.Name.isNotNull())&(df4.Age.isNotNull())).show()\n",
        "print('Using fill()')\n",
        "df4.na.fill({'Name':'unknown','Age':\"\"}).show()\n",
        "print('Using drop()')\n",
        "df4.na.drop().show()\n",
        "\n"
      ],
      "metadata": {
        "id": "8BKYEYrKjmLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Join**"
      ],
      "metadata": {
        "id": "QyCdIeybzXDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df = spark.read \\\n",
        "        .option('inferSchema', 'true') \\\n",
        "        .option('header', 'true') \\\n",
        "        .csv('/content/emptbl.csv')\n",
        "\n",
        "emp_df.show()\n",
        "\n",
        "dept_df = spark.read \\\n",
        "        .option('inferSchema', 'true') \\\n",
        "        .option('header', 'true') \\\n",
        "        .csv('/content/depttbl.csv')\n",
        "\n",
        "dept_df.show()\n",
        "print(\"inner join\")\n",
        "emp_df.join(dept_df,emp_df.deptno == dept_df.deptno,\"inner\").show()\n",
        "print(\"left join\")\n",
        "emp_df.join(dept_df,emp_df.deptno == dept_df.deptno,\"left\").show()\n",
        "print(\"right join\")\n",
        "emp_df.join(dept_df,emp_df.deptno == dept_df.deptno,\"right\").show()\n",
        "print(\"full join\")\n",
        "emp_df.join(dept_df,emp_df.deptno == dept_df.deptno,\"full\").show()"
      ],
      "metadata": {
        "id": "A2UwNXcmyFHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shared Variables - Accumulator, broadcast"
      ],
      "metadata": {
        "id": "zbCcAolBz6Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bdcast=spark.sparkContext.broadcast(['abc','pqr','iuo'])\n",
        "data=bdcast.value[1]\n",
        "print('broadcast value- ',data)\n",
        "\n",
        "accum=spark.sparkContext.accumulator(10)\n",
        "rdd=spark.sparkContext.parallelize([1,2,3,4])\n",
        "rdd.foreach(lambda x:accum.add(x))\n",
        "print('accumulator value- ',accum.value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piCk-VbT0ASR",
        "outputId": "b06a3549-4790-43b1-d4a9-0fa1161ce58e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "broadcast value-  pqr\n",
            "accumulator value-  20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repartiton and Coalesce"
      ],
      "metadata": {
        "id": "PKda7AvL1frm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df = spark.read \\\n",
        "        .option('inferSchema', 'true') \\\n",
        "        .option('header', 'true') \\\n",
        "        .csv('/content/walmart_stock.csv')\n",
        "\n",
        "print('number of partition 1- ',stock_df.rdd.getNumPartitions())\n",
        "stock_df = stock_df.repartition(6)\n",
        "print('number of partition 2 after repartition-',stock_df.rdd.getNumPartitions())\n",
        "stock_df = stock_df.coalesce(3)\n",
        "print('number of partition 3 after coalesce-',stock_df.rdd.getNumPartitions())\n",
        "stock_df.show()"
      ],
      "metadata": {
        "id": "6uiYbOZR1jLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Windows Function on Dataframe like Rank, Dense_rank, row_number"
      ],
      "metadata": {
        "id": "sjSl7ma53MVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number,dense_rank,rank, to_timestamp, to_date\n",
        "\n",
        "emp_df = spark.read \\\n",
        "        .option('inferSchema', 'true') \\\n",
        "        .option('header', 'true') \\\n",
        "        .csv('/content/emptbl.csv')\n",
        "\n",
        "# emp_df.show()\n",
        "emp_df.printSchema()\n",
        "windowSpec  = Window.partitionBy(\"deptno\").orderBy(\"sal\")\n",
        "\n",
        "emp_df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
        "      .withColumn(\"dense_rank\",dense_rank().over(windowSpec))\\\n",
        "      .withColumn(\"rank\",rank().over(windowSpec))\\\n",
        "      .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZlAT96Z3lk9",
        "outputId": "a4735cef-2508-490a-b78f-8240955010f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- empno: integer (nullable = true)\n",
            " |-- ename: string (nullable = true)\n",
            " |-- job: string (nullable = true)\n",
            " |-- mgr: integer (nullable = true)\n",
            " |-- hiredate: string (nullable = true)\n",
            " |-- sal: integer (nullable = true)\n",
            " |-- comm: integer (nullable = true)\n",
            " |-- deptno: integer (nullable = true)\n",
            "\n",
            "+-----+------+---------+----+----------------+----+----+------+----------+----------+----+\n",
            "|empno| ename|      job| mgr|        hiredate| sal|comm|deptno|row_number|dense_rank|rank|\n",
            "+-----+------+---------+----+----------------+----+----+------+----------+----------+----+\n",
            "| 7369| SMITH|    CLERK|7902|13-06-1993 00:00| 800|   0|    20|         1|         1|   1|\n",
            "| 7876| ADAMS|    CLERK|7788|04-06-1999 00:00|1100|null|    20|         2|         2|   2|\n",
            "| 7566| JONES|  MANAGER|7839|31-10-1995 00:00|2975|null|    20|         3|         3|   3|\n",
            "| 7788| SCOTT|  ANALYST|7566|05-03-1996 00:00|3000|null|    20|         4|         4|   4|\n",
            "| 7902|  FORD|  ANALYST|7566|05-12-1997 00:00|3000|null|    20|         5|         4|   4|\n",
            "| 7902|  FORD|  ANALYST|7566|05-12-1997 00:00|3000|null|    20|         6|         4|   4|\n",
            "| 7934|MILLER|    CLERK|7782|21-01-2000 00:00|1300|null|    10|         1|         1|   1|\n",
            "| 7782| CLARK|  MANAGER|7839|14-05-1993 00:00|2450|null|    10|         2|         2|   2|\n",
            "| 7839|  KING|PRESIDENT|null|09-06-1990 00:00|5000|   0|    10|         3|         3|   3|\n",
            "| 7900| JAMES|    CLERK|7698|23-06-2000 00:00| 950|null|    30|         1|         1|   1|\n",
            "| 7900| JAMES|    CLERK|7698|23-06-2000 00:00| 950|null|    30|         2|         1|   1|\n",
            "| 7521|  WARD| SALESMAN|7698|26-03-1996 00:00|1250| 500|    30|         3|         2|   3|\n",
            "| 7654|MARTIN| SALESMAN|7698|05-12-1998 00:00|1250|1400|    30|         4|         2|   3|\n",
            "| 7844|TURNER| SALESMAN|7698|04-06-1995 00:00|1500|   0|    30|         5|         3|   5|\n",
            "| 7499| ALLEN| SALESMAN|7698|15-08-1998 00:00|1600| 300|    30|         6|         4|   6|\n",
            "| 7698| BLAKE|  MANAGER|7839|11-06-1992 00:00|2850|null|    30|         7|         5|   7|\n",
            "+-----+------+---------+----+----------------+----+----+------+----------+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partition and Bucketing on Dataframe"
      ],
      "metadata": {
        "id": "qV1Nr-de5rhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df = spark.read \\\n",
        "        .option('inferSchema', 'true') \\\n",
        "        .option('header', 'true') \\\n",
        "        .csv('/content/emptbl.csv')\n",
        "\n",
        "# emp_df.show()\n",
        "emp_df.printSchema()\n",
        "emp_df.write\\\n",
        "      .partitionBy(\"deptno\") \\\n",
        "      .mode(\"overwrite\") \\\n",
        "      .csv(\"partitions/partitionBy.csv\", header=True)\n",
        "\n",
        "emp_df.write\\\n",
        "      .bucketBy(3, 'deptno') \\\n",
        "      .mode(\"overwrite\") \\\n",
        "      .saveAsTable('bucketed', format='csv')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qy1FpSdF5xP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "spark sql"
      ],
      "metadata": {
        "id": "mziQU1wKCCE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df = spark.read \\\n",
        "        .option('inferSchema', 'true') \\\n",
        "        .option('header', 'true') \\\n",
        "        .csv('/content/emptbl.csv')\n",
        "\n",
        "\n",
        "spark.catalog.dropTempView(\"emp\")\n",
        "emp_df.createTempView(\"emp\")\n",
        "spark.sql('select * from emp').show()\n"
      ],
      "metadata": {
        "id": "F_CIUjUoCFqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write parquet and ORC file"
      ],
      "metadata": {
        "id": "iv7IKwewJryi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df = spark.read \\\n",
        "        .option('inferSchema', 'true') \\\n",
        "        .option('header', 'true') \\\n",
        "        .csv('/content/emptbl.csv')\n",
        "\n",
        "\n",
        "emp_df.write.mode('overwrite').parquet(\"parquet_output/emp.parquet\")\n",
        "\n",
        "\n",
        "emp_df.write.mode('overwrite').format(\"orc\").save(\"orc/emp_orc.orc\")\n"
      ],
      "metadata": {
        "id": "TY2kEMWCJrAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove duplicate records"
      ],
      "metadata": {
        "id": "5uLfSSFoLOik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df = spark.read \\\n",
        "        .option('inferSchema', 'true') \\\n",
        "        .option('header', 'true') \\\n",
        "        .csv('/content/bad_rec_file.csv')\n",
        "\n",
        "emp_df.show()\n",
        "print('removing duplicates using drop_duplicates')\n",
        "emp_df.drop_duplicates().show()\n",
        "print('removing duplicates using distinct')\n",
        "emp_df.distinct().show()"
      ],
      "metadata": {
        "id": "jyv1LEPZLSud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading JSON file"
      ],
      "metadata": {
        "id": "w7vpQQkSN3qC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode,col,from_json\n",
        "from pyspark.sql.types import MapType,StringType\n",
        "json_df=spark.read.option('multiline','true')\\\n",
        ".json('/content/sample_json.json')\n",
        "\n",
        "json_df.show()\n",
        "json_df.printSchema()\n",
        "final_df=json_df.withColumn('contact no',explode('phoneNumbers'))\\\n",
        "                .withColumn('contact_no',col('contact no.number'))\\\n",
        "                .withColumn('contact_type',col('contact no.type'))\\\n",
        "                .drop('contact no','phoneNumbers')\\\n",
        "                .withColumn('city',col(\"address.city\"))\\\n",
        "                .withColumn('state',col(\"address.state\"))\\\n",
        "                .withColumn('streetAddress',col(\"address.streetAddress\"))\\\n",
        "                .drop('address')\n",
        "\n",
        "final_df.printSchema()\n",
        "final_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhlDoEw0N69g",
        "outputId": "e07ddd09-5fbb-4f73-eb8e-fec7e2b32027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---+---------+------+--------+--------------------+\n",
            "|             address|age|firstName|gender|lastName|        phoneNumbers|\n",
            "+--------------------+---+---------+------+--------+--------------------+\n",
            "|[San Diego, CA, 101]| 28|      Joe|  male| Jackson|[[7349282382, home]]|\n",
            "|[dfsdf dsfsdf, LA...| 35|      Dan|female|     Leo|[[854282352, offi...|\n",
            "+--------------------+---+---------+------+--------+--------------------+\n",
            "\n",
            "root\n",
            " |-- address: struct (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            " |    |-- state: string (nullable = true)\n",
            " |    |-- streetAddress: string (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- firstName: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- lastName: string (nullable = true)\n",
            " |-- phoneNumbers: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- number: string (nullable = true)\n",
            " |    |    |-- type: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- firstName: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- lastName: string (nullable = true)\n",
            " |-- contact_no: string (nullable = true)\n",
            " |-- contact_type: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- streetAddress: string (nullable = true)\n",
            "\n",
            "+---+---------+------+--------+----------+------------+------------+-----+-------------+\n",
            "|age|firstName|gender|lastName|contact_no|contact_type|        city|state|streetAddress|\n",
            "+---+---------+------+--------+----------+------------+------------+-----+-------------+\n",
            "| 28|      Joe|  male| Jackson|7349282382|        home|   San Diego|   CA|          101|\n",
            "| 35|      Dan|female|     Leo| 854282352|      office|dfsdf dsfsdf|   LA|          102|\n",
            "+---+---------+------+--------+----------+------------+------------+-----+-------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}